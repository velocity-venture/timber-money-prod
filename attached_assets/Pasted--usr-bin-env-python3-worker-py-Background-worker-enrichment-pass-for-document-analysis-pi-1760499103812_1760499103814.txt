#!/usr/bin/env python3
"""
worker.py
Background worker & enrichment pass for document analysis pipeline.
Single-file, opinionated, and dependency-light. Designed for Replit.
"""

import os
import time
import json
import re
import traceback
from datetime import datetime, timedelta

# External libs:
# pip install sqlalchemy psycopg2-binary boto3 pytesseract pillow pdf2image requests python-dotenv
from sqlalchemy import create_engine, text
from sqlalchemy.exc import OperationalError
import boto3
from botocore.exceptions import BotoCoreError, ClientError
from PIL import Image
import pytesseract
from pdf2image import convert_from_path

# ---------- CONFIG ----------
DATABASE_URL = os.getenv("DATABASE_URL")  # e.g. postgres://user:pass@host:5432/db
S3_BUCKET = os.getenv("S3_BUCKET")        # optional: s3 bucket for file storage
S3_REGION = os.getenv("S3_REGION", "us-east-1")
AWS_USE = bool(os.getenv("AWS_USE", ""))  # set to "1" if using S3/AWS
OCR_FALLBACK_API = os.getenv("OCR_FALLBACK_API")  # optional external OCR endpoint
POLL_INTERVAL = int(os.getenv("POLL_INTERVAL", "6"))  # seconds between polls
BATCH_SIZE = int(os.getenv("BATCH_SIZE", "5"))
MAX_RETRIES = int(os.getenv("MAX_RETRIES", "3"))
ANOMALY_AMOUNT_THRESHOLD = float(os.getenv("ANOMALY_AMOUNT_THRESHOLD", "5000.0"))

# Merchant normalization map (example)
MERCHANT_MAP = {
    "walmart inc": "Walmart",
    "wal-mart": "Walmart",
    "starbucks coffee": "Starbucks",
    "amazon.com": "Amazon",
}

# Category map by keyword
CATEGORY_KEYWORDS = {
    "grocer": ["grocery", "supermarket", "walmart", "safeway", "kroger"],
    "dining": ["coffee", "restaurant", "cafe", "starbucks", "diner"],
    "travel": ["airlines", "uber", "lyft", "hotel", "car rental"],
    "utilities": ["electric", "water", "gas bill", "utility"],
    "shopping": ["amazon", "target", "walmart", "mall"],
}

# ---------- HELPERS ----------
def log(*args):
    print(f"[{datetime.utcnow().isoformat()}]", *args)

def connect_db(url):
    return create_engine(url, pool_pre_ping=True, future=True)

def s3_client():
    return boto3.client("s3", region_name=S3_REGION)

def download_from_s3(s3_key, local_path):
    client = s3_client()
    client.download_file(S3_BUCKET, s3_key, local_path)
    return local_path

def ocr_image_pil(pil_img):
    try:
        text = pytesseract.image_to_string(pil_img)
        return text
    except Exception:
        return ""

def ocr_pdf_local(pdf_path):
    try:
        pages = convert_from_path(pdf_path, dpi=200)
        texts = []
        for p in pages:
            texts.append(pytesseract.image_to_string(p))
        return "\n".join(texts)
    except Exception:
        return ""

def ocr_fallback_api(file_bytes):
    if not OCR_FALLBACK_API:
        return ""
    try:
        import requests
        resp = requests.post(OCR_FALLBACK_API, files={"file": ("file", file_bytes)})
        if resp.status_code == 200:
            return resp.text
    except Exception:
        return ""
    return ""

def detect_doc_type(text):
    t = text.lower()
    if re.search(r"\b(invoice|invoice #|invoice no|invoice number)\b", t):
        return "invoice"
    if re.search(r"\b(receipt|amount paid|total paid)\b", t):
        return "receipt"
    if re.search(r"\b(statement of account|account statement|ending balance|opening balance)\b", t):
        return "statement"
    # fallback heuristic: if many line-items/dates -> statement or invoice
    if len(re.findall(r"\$\s*\d", t)) > 3 or len(re.findall(r"\b\d{2}/\d{2}/\d{2,4}\b", t)) > 3:
        return "statement"
    return "unknown"

def find_date(text):
    m = re.search(r"(\d{4}-\d{2}-\d{2})|(\d{2}/\d{2}/\d{2,4})", text)
    return m.group(0) if m else None

def find_total(text):
    # common total patterns
    m = re.search(r"(total due|balance due|amount due|total)\s*[:\-\s]*\$\s*([0-9\.,]+)", text, re.I)
    if m:
        return float(m.group(2).replace(",", ""))
    m2 = re.search(r"\$\s*([0-9\.,]+)\s*(?:total|subtotal)?", text)
    if m2:
        return float(m2.group(1).replace(",", ""))
    # fallback: largest $ found
    all_amounts = re.findall(r"\$\s*([0-9\.,]+)", text)
    if all_amounts:
        nums = [float(a.replace(",", "")) for a in all_amounts]
        return max(nums)
    return None

def normalize_merchant(raw_name):
    if not raw_name:
        return None
    s = raw_name.lower().strip()
    for k, v in MERCHANT_MAP.items():
        if k in s:
            return v
    # strip punctuation and reduce
    s2 = re.sub(r"[^\w\s]", "", s)
    return s2.title()

def categorize_text(text):
    t = text.lower()
    for cat, keys in CATEGORY_KEYWORDS.items():
        for k in keys:
            if k in t:
                return cat
    return "uncategorized"

def anomaly_checks(parsed):
    anomalies = []
    total = parsed.get("total_amount")
    if total is not None and abs(total) >= ANOMALY_AMOUNT_THRESHOLD:
        anomalies.append({"type": "large_amount", "total": total})
    # weird negative totals
    if total is not None and total < 0:
        anomalies.append({"type": "negative_total", "total": total})
    return anomalies

# ---------- SIMPLE PARSERS ----------
def parse_receipt(text):
    parsed = {}
    parsed["type"] = "receipt"
    parsed["date"] = find_date(text)
    parsed["total_amount"] = find_total(text)
    parsed["merchant_raw"] = re.search(r"^(.*)$", text.strip().splitlines()[0]).group(0) if text.strip() else None
    parsed["merchant"] = normalize_merchant(parsed.get("merchant_raw"))
    parsed["category"] = categorize_text(text)
    parsed["confidence"] = 0.75  # default heuristic confidence
    return parsed

def parse_invoice(text):
    parsed = {}
    parsed["type"] = "invoice"
    parsed["invoice_number"] = (re.search(r"(invoice\s*(#|no\.?)\s*[:#]?\s*(\S+))", text, re.I) or re.search(r"invoice\s*[:\s]*(\S+)", text, re.I))
    parsed["invoice_number"] = parsed["invoice_number"].groups()[-1] if parsed["invoice_number"] else None
    parsed["date"] = find_date(text)
    parsed["total_amount"] = find_total(text)
    parsed["merchant_raw"] = re.search(r"^(.*)$", text.strip().splitlines()[0]).group(0) if text.strip() else None
    parsed["merchant"] = normalize_merchant(parsed.get("merchant_raw"))
    parsed["category"] = categorize_text(text)
    parsed["confidence"] = 0.7
    return parsed

def parse_statement(text):
    parsed = {}
    parsed["type"] = "statement"
    parsed["period_start"] = None
    parsed["period_end"] = None
    parsed["date"] = find_date(text)
    parsed["total_amount"] = find_total(text)
    parsed["merchant"] = None
    parsed["category"] = "statement"
    parsed["confidence"] = 0.6
    # optional: extract transactions (very naive)
    lines = text.splitlines()
    txs = []
    for line in lines:
        m = re.search(r"(\d{2}/\d{2}/\d{2,4})\s+(.+?)\s+\$?\s*([0-9\.,]+)", line)
        if m:
            txs.append({
                "date": m.group(1),
                "description": m.group(2).strip(),
                "amount": float(m.group(3).replace(",", ""))
            })
    parsed["transactions"] = txs
    return parsed

# ---------- MAIN PROCESSING ----------
def process_document_row(row, engine):
    """
    Expected minimal schema for `documents` table:
    id (uuid/int), s3_key OR local_path, status, attempts (int), parsed_json (jsonb), created_at
    """
    doc_id = row["id"]
    s3_key = row.get("s3_key")
    local_path = row.get("local_path")
    file_path = None
    tmp_local = f"/tmp/doc_{doc_id}"
    try:
        log("Processing doc", doc_id, "status:", row.get("status"))
        # download if S3
        if AWS_USE and s3_key:
            download_from_s3(s3_key, tmp_local)
            file_path = tmp_local
        elif local_path and os.path.exists(local_path):
            file_path = local_path
        else:
            log("No file available locally; skipping doc", doc_id)
            # mark errored
            engine.execute(text("UPDATE documents SET status='errored', last_error=:e WHERE id=:id"),
                           {"e": "missing_file", "id": doc_id})
            return

        text_content = ""
        # handle pdf vs image heuristics
        if file_path.lower().endswith(".pdf"):
            log("Running PDF OCR for", doc_id)
            text_content = ocr_pdf_local(file_path)
        else:
            try:
                img = Image.open(file_path)
                text_content = ocr_image_pil(img)
            except Exception:
                # fallback: call external OCR API
                with open(file_path, "rb") as fh:
                    text_content = ocr_fallback_api(fh.read())

        # If OCR returned little, try fallback
        if len(text_content.strip()) < 30:
            log("OCR low-confidence; trying fallback API for", doc_id)
            with open(file_path, "rb") as fh:
                text_content = ocr_fallback_api(fh.read())

        doc_type = detect_doc_type(text_content)
        if doc_type == "receipt":
            parsed = parse_receipt(text_content)
        elif doc_type == "invoice":
            parsed = parse_invoice(text_content)
        elif doc_type == "statement":
            parsed = parse_statement(text_content)
        else:
            parsed = {
                "type": "unknown",
                "raw_text_snippet": text_content[:200],
                "confidence": 0.35
            }

        # Enrich
        parsed["merchant_normalized"] = normalize_merchant(parsed.get("merchant") or parsed.get("merchant_raw"))
        parsed["anomalies"] = anomaly_checks(parsed)
        parsed["parsed_at"] = datetime.utcnow().isoformat()

        # Decide status based on confidence or anomalies
        status = "parsed"
        if parsed.get("confidence", 0) < 0.5 or parsed["anomalies"]:
            status = "needs_review"

        # Save parsed JSON back
        engine.execute(text("""
            UPDATE documents
               SET parsed_json = :p,
                   status = :status,
                   attempts = COALESCE(attempts,0) + 1,
                   processed_at = NOW()
             WHERE id = :id
        """), {"p": json.dumps(parsed), "status": status, "id": doc_id})

        log("Processed", doc_id, "->", status)
    except Exception as e:
        tb = traceback.format_exc()
        log("Error processing doc", doc_id, str(e))
        # increment attempts and set backoff/errored when MAX reached
        engine.execute(text("""
            UPDATE documents
               SET attempts = COALESCE(attempts,0) + 1,
                   last_error = :err,
                   last_error_at = NOW()
             WHERE id = :id
        """), {"err": str(e)[:2000], "id": doc_id})
        # fetch attempts
        attempts = engine.execute(text("SELECT attempts FROM documents WHERE id=:id"), {"id": doc_i